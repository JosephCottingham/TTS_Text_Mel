# This is the hyperparameter configuration file for Tacotron2 v1.
# Please make sure this is adjusted for the LJSpeech dataset. If you want to
# apply to the other dataset, you might need to carefully change some parameters.
# This configuration performs 200k iters but 65k iters is enough to get a good models.

###########################################################
#                FEATURE EXTRACTION SETTING               #
###########################################################
hop_size: 256            # Hop size.
format: "npy"


###########################################################
#                  DATA LOADER SETTING                    #
###########################################################
dataset_manager_params:
    reduction_factor: 1
    batch_size: 1             # Batch size for each GPU with assuming that gradient_accumulation_steps == 1.
    remove_short_samples: true # Whether to remove samples the length of which are less than batch_max_steps.
    allow_cache: true          # Whether to allow cache in dataset. If true, it requires cpu memory.
    mel_length_threshold: 32   # remove all targets has mel_length <= 32 
    is_shuffle: false           # shuffle dataset after each epoch.
    use_fixed_shapes: true     # use_fixed_shapes for training (2x speed-up)
                           # refer (https://github.com/dathudeptrai/TensorflowTTS/issues/34#issuecomment-642309118)

###########################################################
#             OPTIMIZER & SCHEDULER SETTING               #
###########################################################
generator_optimizer_params:
    learning_rate: 0.0001             # Generator's learning rate.
    beta_1: 0.5
    beta_2: 0.9
    
discriminator_optimizer_params:
    learning_rate: 0.0001            # Discriminator's learning rate.
    beta_1: 0.5
    beta_2: 0.9

gradient_accumulation_steps: 1

###########################################################
#                    INTERVAL SETTING                     #
###########################################################
train_max_steps: 2000                 # Number of training steps.
train_max_epochs: 20
save_interval_steps: 500               # Interval steps to save checkpoint.
eval_interval_steps: 250                # Interval steps to evaluate the network.
log_interval_steps: 50                # Interval steps to record the training log.
start_schedule_teacher_forcing: 200001  # don't need to apply schedule teacher forcing.
start_ratio_value: 0.5                  # start ratio of scheduled teacher forcing.
schedule_decay_steps: 50000             # decay step scheduled teacher forcing.
end_ratio_value: 0.0                    # end ratio of scheduled teacher forcing.
discriminator_train_start_steps: 0     # step to start training discriminator.

###########################################################
#                     OTHER SETTING                       #
###########################################################
num_save_intermediate_results: 1  # Number of results to be saved as intermediate results.

###########################################################
#               ADVERSARIAL LOSS SETTING                  #
###########################################################
lambda_feat_match: 10.0

###########################################################
#       DISCRIMINATOR NETWORK ARCHITECTURE SETTING        #
###########################################################

###########################################################
#       GENERATOR NETWORK ARCHITECTURE SETTING        #
###########################################################
model_type: "tacotron2-cus"

generator_params:
    n_mels: 80 # Output channels for generator